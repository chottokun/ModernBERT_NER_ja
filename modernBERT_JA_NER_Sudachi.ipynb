{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMFt7BXd5KQdracrxv53RiF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chottokun/ModernBERT_NER_ja/blob/main/modernBERT_JA_NER_Sudachi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf9300d7"
      },
      "source": [
        "## ModernBERT NER の整理\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ModernBERT ベースモデル・HFレポジトリ名設定\n",
        "\n",
        "# ベースモデルとなるモデル　hugging\n",
        "model_name = \"cl-nagoya/ruri-v3-130m\"\n",
        "\n",
        "# Hugging Faceユーザー名または組織名 (例: \"your-username\" または \"your-org\")\n",
        "# ログインしているユーザーのリポジトリに作成する場合、ユーザー名または組織名は不要です。\n",
        "# repo_id = \"your-username/japanese-ner-wikipedia-sudachi\" # ユーザー名を指定する場合\n",
        "repo_id = \"Chottokun/ruri-v3-pt-130m_ner_wikipedia\" # ログインユーザーのリポジトリに作成する場合\n",
        "\n",
        "# 学習条件\n",
        "EARLY_STOP = True # False に設定すると Early Stopping は無効になります\n",
        "NUM_TRAIN_EPOCHS = 50 # Early Stoppingを使用する場合、num_train_epochsは最大エポック数となる\n"
      ],
      "metadata": {
        "id": "c74f26VEOTHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08811388"
      },
      "source": [
        "## Hugging Face Hub へのログイン\n",
        "\n",
        "### Subtask:\n",
        "Hugging Face Hub に HF-TOKEN を使用してログインします。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0c70435"
      },
      "source": [
        "**Reasoning**:\n",
        "Log in to the Hugging Face Hub programmatically using the HF-TOKEN stored as a Colab secret for authentication."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "720bdd23"
      },
      "source": [
        "from huggingface_hub import notebook_login\n",
        "from google.colab import userdata\n",
        "\n",
        "# HF-TOKEN を Colab の Secrets から取得してログイン\n",
        "try:\n",
        "    hf_token = userdata.get('HF_TOKEN')\n",
        "    if hf_token:\n",
        "        # notebook_login() # これによりトークン入力プロンプトが表示されるか、既にログイン済みならスキップされる\n",
        "        # Alternatively, you can directly login with the token if notebook_login is not interactive enough\n",
        "        from huggingface_hub import login\n",
        "        login(token=hf_token)\n",
        "    else:\n",
        "        print(\"Colab Secrets に 'HF_TOKEN' が設定されていません。設定してください。\")\n",
        "except Exception as e:\n",
        "    print(f\"Hugging Face Hub ログイン中にエラーが発生しました: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6789b435"
      },
      "source": [
        "## ライブラリのインストール\n",
        "\n",
        "### Subtask:\n",
        "必要なライブラリ（`transformers`, `datasets`, `SudachiPy`, `SudachiDict_core`, `spacy-alignments`, `seqeval`, `evaluate`）をインストールします。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21fd11c5"
      },
      "source": [
        "!pip install -U -q transformers datasets SudachiPy SudachiDict_core spacy-alignments seqeval evaluate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e7e4889"
      },
      "source": [
        "## データセットとトークナイザーの準備\n",
        "\n",
        "### Subtask:\n",
        "データセットをロードし、モデルのトークナイザー、Sudachiトークナイザー、およびラベルのマッピングを準備します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8229ecee"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "from sudachipy import Dictionary, SplitMode\n",
        "\n",
        "# モデルとトークナイザーの準備\n",
        "# model_name = \"cl-nagoya/ruri-v3-130m\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Sudachi辞書の準備\n",
        "dict_obj = Dictionary()\n",
        "tokenizer_obj = dict_obj.create()\n",
        "\n",
        "# カスタムSudachiトークナイザー関数の定義\n",
        "def sudachi_tokenizer(text):\n",
        "    # A mode for short units\n",
        "    tokens = [m.surface() for m in tokenizer_obj.tokenize(text, SplitMode.A)]\n",
        "    return tokens\n",
        "\n",
        "# データセットの準備\n",
        "dataset = load_dataset(\"stockmark/ner-wikipedia-dataset\")\n",
        "\n",
        "# ラベルのマッピング\n",
        "label_list = [\"O\", \"B-人名\", \"I-人名\", \"B-法人名\", \"I-法人名\", \"B-政治的組織名\", \"I-政治的組織名\",\n",
        "              \"B-その他の組織名\", \"I-その他の組織名\", \"B-地名\", \"I-地名\", \"B-施設名\", \"I-施設名\",\n",
        "              \"B-製品名\", \"I-製品名\", \"B-イベント名\", \"I-イベント名\"]\n",
        "label2id = {l: i for i, l in enumerate(label_list)}\n",
        "id2label = {i: l for l, i in label2id.items()}\n",
        "\n",
        "print(\"データセット、トークナイザー、ラベルマッピングの準備が完了しました。\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4231a0c"
      },
      "source": [
        "## データ前処理とラベルアライメント\n",
        "\n",
        "### Subtask:\n",
        "Sudachiで分かち書きを行い、Hugging Faceトークナイザーでサブワード化し、`spacy-alignments`を用いてラベルをアライメントします。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36339c09"
      },
      "source": [
        "import spacy_alignments as tokenizations # Import spacy_alignments\n",
        "\n",
        "def tokenize_and_align_labels_sudachi(examples):\n",
        "    all_input_ids = []\n",
        "    all_attention_mask = []\n",
        "    all_labels = []\n",
        "\n",
        "    for i, text in enumerate(examples[\"text\"]):\n",
        "        entities = examples[\"entities\"][i]\n",
        "        # 1. Sudachiで分かち書きを行う\n",
        "        words = sudachi_tokenizer(text)\n",
        "\n",
        "        # 2. 分かち書きした単語列をHugging Faceトークナイザーに入力\n",
        "        #    is_split_into_words=True を使用し、return_offsets_mapping=True でオフセット情報を取得\n",
        "        tokenized_inputs = tokenizer(\n",
        "            words,\n",
        "            is_split_into_words=True,\n",
        "            truncation=True,\n",
        "            return_offsets_mapping=True,\n",
        "            return_attention_mask=True,\n",
        "            add_special_tokens=True # Special tokens are added here\n",
        "        )\n",
        "\n",
        "        input_ids = tokenized_inputs[\"input_ids\"]\n",
        "        attention_mask = tokenized_inputs[\"attention_mask\"]\n",
        "        offset_mapping = tokenized_inputs[\"offset_mapping\"]\n",
        "\n",
        "        # 3. spacy_alignments を使用して単語とサブワードのアライメントを取得\n",
        "        #    Hugging Face tokenizer converts words to subwords, so we need to convert token_ids back to tokens for alignment\n",
        "        #    Exclude special tokens from subwords for alignment when creating subwords_for_alignment\n",
        "        subwords_for_alignment = tokenizer.convert_ids_to_tokens(tokenizer(words, is_split_into_words=True, add_special_tokens=False).input_ids)\n",
        "\n",
        "\n",
        "        words2subwords, subwords2words = tokenizations.get_alignments(words, subwords_for_alignment)\n",
        "\n",
        "\n",
        "        # 4. エンティティラベルをサブワードにアライメント\n",
        "        aligned_labels = [-100] * len(input_ids)\n",
        "\n",
        "        # Create word-level labels first based on original text and entities\n",
        "        word_labels = [\"O\"] * len(words)\n",
        "        # Calculate character spans for each word in the original text\n",
        "        word_char_spans = []\n",
        "        current_char_index = 0\n",
        "        for word_text in words:\n",
        "            start_index = text.find(word_text, current_char_index)\n",
        "            if start_index != -1:\n",
        "                end_index = start_index + len(word_text)\n",
        "                word_char_spans.append((start_index, end_index))\n",
        "                current_char_index = end_index\n",
        "            else:\n",
        "                 word_char_spans.append((-1, -1)) # Should not happen with correct tokenization\n",
        "\n",
        "        # Assign B- or I- labels at the word level\n",
        "        for ent in entities:\n",
        "             ent_start_char, ent_end_char = ent[\"span\"]\n",
        "             ent_type = ent[\"type\"]\n",
        "\n",
        "             for word_index, (word_char_start, word_char_end) in enumerate(word_char_spans):\n",
        "                  if word_char_start != -1 and max(word_char_start, ent_start_char) < min(word_char_end, ent_end_char):\n",
        "                       # Word overlaps with an entity\n",
        "                       if word_char_start >= ent_start_char: # If the word starts at or after the entity start\n",
        "                            is_entity_start = False\n",
        "                            # Check if the previous word is part of the same entity\n",
        "                            if word_index == 0:\n",
        "                                is_entity_start = True\n",
        "                            else:\n",
        "                                prev_word_char_start, prev_word_char_end = word_char_spans[word_index - 1]\n",
        "                                prev_word_in_same_entity = False\n",
        "                                if prev_word_char_start != -1:\n",
        "                                     for prev_ent in entities:\n",
        "                                          prev_ent_start, prev_ent_end = prev_ent[\"span\"]\n",
        "                                          prev_ent_type = prev_ent[\"type\"]\n",
        "                                          # Check if the previous word's character span overlaps with the same entity type\n",
        "                                          if max(prev_word_char_start, prev_ent_start) < min(prev_word_char_end, prev_ent_end) and prev_ent_type == ent_type:\n",
        "                                                prev_word_in_same_entity = True\n",
        "                                                break\n",
        "                                if not prev_word_in_same_entity:\n",
        "                                     is_entity_start = True\n",
        "\n",
        "                            word_labels[word_index] = \"B-\" + ent_type if is_entity_start else \"I-\" + ent_type\n",
        "\n",
        "\n",
        "        # Transfer word-level labels to subword-level labels using alignments\n",
        "        for subword_index in range(len(input_ids)):\n",
        "            if tokenizer.convert_ids_to_tokens(input_ids[subword_index]) in tokenizer.all_special_tokens:\n",
        "                 aligned_labels[subword_index] = -100\n",
        "            else:\n",
        "                 # Get the word index(es) aligned to this subword (adjusting for special tokens)\n",
        "                 # The alignment from spacy_alignments is based on subwords_for_alignment (without special tokens)\n",
        "                 # We need to map the current subword_index (from input_ids with special tokens)\n",
        "                 # to the corresponding index in subwords_for_alignment.\n",
        "                 # Assuming special tokens are only at the beginning ([CLS]) and end ([SEP]),\n",
        "                 # the subword at index `k` in input_ids (where k > 0 and k < len(input_ids) - 1)\n",
        "                 # corresponds to the subword at index `k-1` in subwords_for_alignment.\n",
        "                 subword_alignment_index = subword_index - 1 # Adjust for [CLS] token at the beginning\n",
        "\n",
        "                 if subword_alignment_index >= 0 and subword_alignment_index < len(subwords2words):\n",
        "                      aligned_word_indices = subwords2words[subword_alignment_index]\n",
        "\n",
        "                      if aligned_word_indices:\n",
        "                           # Take the label of the first aligned word\n",
        "                           first_aligned_word_index = aligned_word_indices[0]\n",
        "                           if first_aligned_word_index < len(word_labels):\n",
        "                                word_level_label = word_labels[first_aligned_word_index]\n",
        "\n",
        "                                if word_level_label != \"O\":\n",
        "                                     # If this subword is the first subword of the first aligned word, assign B-\n",
        "                                     # Otherwise, assign I-\n",
        "                                     is_first_subword_of_word = (first_aligned_word_index < len(words2subwords)) and (subword_alignment_index in words2subwords[first_aligned_word_index]) and (words2subwords[first_aligned_word_index][0] == subword_alignment_index)\n",
        "\n",
        "\n",
        "                                     if is_first_subword_of_word:\n",
        "                                          aligned_labels[subword_index] = label2id[word_level_label]\n",
        "                                     else:\n",
        "                                          # Ensure I- label matches the B- label type\n",
        "                                          if word_level_label.startswith(\"B-\"):\n",
        "                                               aligned_labels[subword_index] = label2id[\"I-\" + word_level_label.split(\"-\")[1]]\n",
        "                                          else:\n",
        "                                               aligned_labels[subword_index] = label2id[word_level_label] # It's already an I- label at word level\n",
        "                                else:\n",
        "                                     aligned_labels[subword_index] = label2id[\"O\"]\n",
        "                      else:\n",
        "                           # Subword aligned to no word (should not happen with is_split_into_words=True and proper alignment)\n",
        "                           aligned_labels[subword_index] = label2id[\"O\"] # Default to O\n",
        "                 else:\n",
        "                      # This case handles the last special token ([SEP]) and any potential padding tokens\n",
        "                      aligned_labels[subword_index] = -100\n",
        "\n",
        "\n",
        "        all_input_ids.append(input_ids)\n",
        "        all_attention_mask.append(attention_mask)\n",
        "        all_labels.append(aligned_labels)\n",
        "\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": all_input_ids,\n",
        "        \"attention_mask\": all_attention_mask,\n",
        "        \"labels\": all_labels\n",
        "    }\n",
        "\n",
        "\n",
        "# データセットに前処理を適用 (分割前にmapを適用)\n",
        "tokenized_datasets_sudachi = dataset.map(\n",
        "    tokenize_and_align_labels_sudachi,\n",
        "    batched=True,\n",
        "    remove_columns=dataset[\"train\"].column_names,\n",
        ")\n",
        "\n",
        "# \"train\" 分割から 10% を検証用として作成 (map処理後に分割)\n",
        "split_datasets_sudachi = tokenized_datasets_sudachi[\"train\"].train_test_split(test_size=0.1)\n",
        "\n",
        "print(\"データ前処理とラベルアライメントが完了しました。\")\n",
        "\n",
        "# 確認のために最初の数件を表示\n",
        "df_sudachi = split_datasets_sudachi[\"train\"].select(range(5)).to_pandas()\n",
        "display(df_sudachi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0ea2ee9"
      },
      "source": [
        "## トレーニングの実行\n",
        "\n",
        "### Subtask:\n",
        "更新したデータセットでモデルのトレーニングを実行します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "802d165c"
      },
      "source": [
        "import torch\n",
        "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer, get_linear_schedule_with_warmup, DataCollatorForTokenClassification\n",
        "import numpy as np\n",
        "import evaluate\n",
        "from transformers.trainer_callback import EarlyStoppingCallback\n",
        "\n",
        "# Early Stopping を有効にするかのフラグ\n",
        "EARLY_STOP = True # False に設定すると Early Stopping は無効になります\n",
        "\n",
        "# モデルの準備\n",
        "# model_name = \"sbintuitions/modernbert-ja-130m\"\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=len(label_list))\n",
        "\n",
        "# 評価指標の準備\n",
        "metric = evaluate.load(\"seqeval\")\n",
        "\n",
        "# 評価指標の計算関数\n",
        "def compute_metrics(eval_preds):\n",
        "    logits, labels = eval_preds\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    # Remove ignored index (special tokens) and convert to labels\n",
        "    true_labels = [[label_list[l] for l in label if l != -100] for label in labels]\n",
        "    true_predictions = [\n",
        "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": all_metrics[\"overall_precision\"],\n",
        "        \"recall\": all_metrics[\"overall_recall\"],\n",
        "        \"f1\": all_metrics[\"overall_f1\"],\n",
        "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
        "    }\n",
        "\n",
        "# トレーニングの設定\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results_sudachi\",\n",
        "    eval_strategy=\"epoch\", # Early Stoppingにはevaluation_strategyが\"epoch\"または\"steps\"である必要がある\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    logging_dir=\"./logs_sudachi\",\n",
        "    num_train_epochs=NUM_TRAIN_EPOCHS, # Early Stoppingを使用する場合、num_train_epochsは最大エポック数となる\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True, # Early Stoppingにはload_best_model_at_endがTrueである必要がある\n",
        "    save_strategy=\"epoch\",\n",
        "    report_to=\"none\",\n",
        "    metric_for_best_model=\"f1\", # 監視する評価指標 (compute_metricsのキーと一致させる)\n",
        "    greater_is_better=True, # 監視する評価指標が大きい方が良いか (f1は大きい方が良い)\n",
        ")\n",
        "\n",
        "# オプティマイザと学習率スケジューラの設定\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=training_args.learning_rate) # Use learning_rate from training_args\n",
        "num_training_steps = len(split_datasets_sudachi[\"train\"]) // training_args.per_device_train_batch_size * training_args.num_train_epochs\n",
        "num_warmup_steps = int(training_args.warmup_ratio * num_training_steps) if training_args.warmup_ratio is not None else training_args.warmup_steps\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=num_warmup_steps,\n",
        "    num_training_steps=num_training_steps\n",
        ")\n",
        "\n",
        "# データコレーターの作成\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
        "\n",
        "# Early Stopping コールバックの設定 (条件付きで追加)\n",
        "callbacks = []\n",
        "if EARLY_STOP:\n",
        "    early_stopping_callback = EarlyStoppingCallback(\n",
        "        early_stopping_patience=3, # 監視指標がこのエポック/ステップ数改善しない場合に停止\n",
        "        early_stopping_threshold=0.0, # 改善とみなす最小の変化量 (0.0でわずかな変化でも改善とみなす)\n",
        "    )\n",
        "    callbacks.append(early_stopping_callback)\n",
        "\n",
        "\n",
        "# Trainerの初期化と学習の実行\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=split_datasets_sudachi[\"train\"],\n",
        "    eval_dataset=split_datasets_sudachi[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer,\n",
        "    optimizers=(optimizer, scheduler),\n",
        "    callbacks=callbacks, # Callbacks リストを渡す\n",
        ")\n",
        "\n",
        "# トレーニングの開始\n",
        "trainer.train()\n",
        "\n",
        "# モデルの保存\n",
        "trainer.save_model(\"./japanese_ner_wikipedia_sudachi\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c32ddf35"
      },
      "source": [
        "## モデルの評価 (詳細)\n",
        "\n",
        "### Subtask:\n",
        "トレーニング済みのモデルをテストデータセットで評価し、詳細な評価指標を表示します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dad3982"
      },
      "source": [
        "# トレーニング済みモデルでの評価を実行\n",
        "evaluation_results = trainer.evaluate(split_datasets_sudachi[\"test\"])\n",
        "\n",
        "# 評価結果を表示\n",
        "print(\"--- 詳細な評価結果 ---\")\n",
        "import pprint # Add import statement\n",
        "pprint.pprint(evaluation_results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4134f3af"
      },
      "source": [
        "## サンプル予測結果の確認\n",
        "\n",
        "### Subtask:\n",
        "テストデータセットからいくつかのサンプルを選び、モデルの予測結果を元の文章、正解ラベルと比較して表示します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7f608bdd"
      },
      "source": [
        "# 実際の予測結果を確認する\n",
        "predicted_labels = []\n",
        "\n",
        "# テストデータセットから10個をサンプリング\n",
        "for i in range(0, 10):\n",
        "    # テストデータセットからサンプルを1つ選択\n",
        "    sample = split_datasets_sudachi[\"test\"][i] # Use split_datasets_sudachi\n",
        "\n",
        "    # Trainerのpredictメソッドはデータセットまたはデータセットのリストを受け取る\n",
        "    # 単一のサンプルに対して予測を行う場合は、データセット形式に合わせる必要がある\n",
        "    # または、pipelineを使用する方が単一テキストの予測には適している\n",
        "\n",
        "    # trainer.predict([sample]) の代わりに pipeline を使用して手軽に予測を行う\n",
        "    # ただし、pipelineはモデルのロードが必要。ここではtrainerオブジェクトが使えるのでtrainer.predictを使う方向で調整\n",
        "    # trainer.predictはBatchConverterを内部で使用するため、単一サンプルをリストで渡すのが適切\n",
        "    predictions = trainer.predict([sample])\n",
        "\n",
        "\n",
        "    # 予測結果を解釈\n",
        "    predicted_label_ids = predictions.predictions.argmax(-1)[0]\n",
        "\n",
        "    # special tokenを除外してラベル名に変換\n",
        "    # sample[\"labels\"] も特殊トークンは-100になっている前提\n",
        "    true_predictions = [\n",
        "        label_list[p_id] for p_id, l_id in zip(predicted_label_ids, sample[\"labels\"]) if l_id != -100\n",
        "    ]\n",
        "    true_labels = [\n",
        "        label_list[l_id] for l_id in sample[\"labels\"] if l_id != -100\n",
        "    ]\n",
        "\n",
        "    # トークンを取得 (特殊トークンを除外)\n",
        "    tokens = tokenizer.convert_ids_to_tokens(sample['input_ids'])\n",
        "    # アライメントされたラベルに対応するトークンのみを取得するため、-100でないラベルに対応するトークンを選ぶ\n",
        "    aligned_tokens = [token for token, l_id in zip(tokens, sample[\"labels\"]) if l_id != -100]\n",
        "\n",
        "\n",
        "    # 元の文章と予測結果を出力\n",
        "    print(\"元の文章トークン:\", ' '.join(aligned_tokens))\n",
        "    print(\"予測ラベル:\", true_predictions)  # special tokenを除外した予測結果を出力\n",
        "    print(\"正解ラベル:\", true_labels) # special tokenを除外した正解ラベルを出力\n",
        "    # 正誤判定はラベル名で行う\n",
        "    print(\"正誤:\", [\"○\" if p == l else \"×\" for p, l in zip(true_predictions, true_labels)])\n",
        "    print(\"---\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fe5689a"
      },
      "source": [
        "## Hugging Face Hub にモデルをプッシュ\n",
        "\n",
        "### Subtask:\n",
        "Hugging Face Hub に新しいリポジトリを作成し、トレーニング済みのモデルファイルをアップロードします。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61f7b804"
      },
      "source": [
        "from huggingface_hub import create_repo, HfApi\n",
        "import os\n",
        "\n",
        "# Hugging Faceユーザー名または組織名 (例: \"your-username\" または \"your-org\")\n",
        "# ログインしているユーザーのリポジトリに作成する場合、ユーザー名または組織名は不要です。\n",
        "# repo_id = \"your-username/japanese-ner-wikipedia-sudachi\" # ユーザー名を指定する場合\n",
        "# repo_id = \"Chottokun/ruri-v3-pt-130m_ner_wikipedia\" # ログインユーザーのリポジトリに作成する場合\n",
        "\n",
        "# リポジトリを作成\n",
        "try:\n",
        "    create_repo(repo_id, repo_type=\"model\", exist_ok=True)\n",
        "    print(f\"リポジトリ '{repo_id}' が作成または既に存在します。\")\n",
        "except Exception as e:\n",
        "    print(f\"リポジトリの作成中にエラーが発生しました: {e}\")\n",
        "\n",
        "\n",
        "# トレーニング済みのモデルディレクトリ\n",
        "local_model_path = \"./japanese_ner_wikipedia\"\n",
        "\n",
        "# モデルファイルをHugging Face Hubにプッシュ\n",
        "# Trainer オブジェクトが push_to_hub メソッドを持っている場合\n",
        "try:\n",
        "    if 'trainer' in locals() and hasattr(trainer, 'push_to_hub'):\n",
        "        print(f\"'{local_model_path}' のモデルファイルを '{repo_id}' にプッシュします...\")\n",
        "        trainer.push_to_hub(repo_id)\n",
        "        print(\"モデルのプッシュが完了しました。\")\n",
        "    else:\n",
        "        print(\"Trainer オブジェクトが見つからないか、push_to_hub メソッドがありません。\")\n",
        "        print(\"代わりに HfApi を使用してファイルをアップロードします。\")\n",
        "\n",
        "        # HfApi を使用してファイルをアップロードする場合\n",
        "        api = HfApi()\n",
        "        # アップロードするファイルリスト (必要に応じて調整)\n",
        "        files_to_upload = [\n",
        "            os.path.join(local_model_path, \"config.json\"),\n",
        "            os.path.join(local_model_path, \"pytorch_model.bin\"),\n",
        "            os.path.join(local_model_path, \"training_args.bin\"),\n",
        "            # Add other necessary files like tokenizer files if they were saved to local_model_path\n",
        "            os.path.join(local_model_path, \"tokenizer.json\"),\n",
        "            os.path.join(local_model_path, \"special_tokens_map.json\"),\n",
        "            os.path.join(local_model_path, \"tokenizer_config.json\"),\n",
        "            os.path.join(local_model_path, \"vocab.txt\"), # If using a vocab.txt based tokenizer\n",
        "            os.path.join(local_model_path, \"spm.model\"), # If using a SentencePiece model\n",
        "        ]\n",
        "\n",
        "        for file_path in files_to_upload:\n",
        "            if os.path.exists(file_path):\n",
        "                try:\n",
        "                    api.upload_file(\n",
        "                        path_or_fileobj=file_path,\n",
        "                        path_in_repo=os.path.basename(file_path),\n",
        "                        repo_id=repo_id,\n",
        "                        repo_type=\"model\",\n",
        "                    )\n",
        "                    print(f\"'{os.path.basename(file_path)}' をアップロードしました。\")\n",
        "                except Exception as e:\n",
        "                    print(f\"ファイルのアップロード中にエラーが発生しました ({os.path.basename(file_path)}): {e}\")\n",
        "            else:\n",
        "                print(f\"ファイル '{file_path}' が見つかりませんでした。スキップします。\")\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"モデルのプッシュ中にエラーが発生しました: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}